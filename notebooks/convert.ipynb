{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76c5523a",
   "metadata": {},
   "source": [
    "# Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad83f483",
   "metadata": {},
   "source": [
    "## Считаем данные в исходном csv формате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb571278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: findspark in ./.local/lib/python3.8/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea29b7",
   "metadata": {},
   "source": [
    "Перезапустить kernel после установки!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbd680b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "416d1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir -p /user/ubuntu/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c75dcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 18:05:47,905 INFO tools.DistCp: Input Options: DistCpOptions{atomicCommit=false, syncFolder=false, deleteMissing=false, ignoreFailures=false, overwrite=false, append=false, useDiff=false, useRdiff=false, fromSnapshot=null, toSnapshot=null, skipCRC=false, blocking=true, numListstatusThreads=0, maxMaps=20, mapBandwidth=0.0, copyStrategy='uniformsize', preserveStatus=[], atomicWorkPath=null, logPath=null, sourceFileListing=null, sourcePaths=[s3a://otus-bucket-b1gcq77orp7o97v8lnkm/], targetPath=/user/ubuntu/data, filtersFile='null', blocksPerChunk=0, copyBufferSize=8192, verboseLog=false, directWrite=false}, sourcePaths=[s3a://otus-bucket-b1gcq77orp7o97v8lnkm/], targetPathExists=true, preserveRawXattrsfalse\n",
      "2025-12-03 18:05:48,049 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-12-03 18:05:48,168 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-12-03 18:05:48,169 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
      "2025-12-03 18:05:48,301 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
      "2025-12-03 18:05:48,302 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
      "2025-12-03 18:05:48,302 INFO impl.MetricsSystemImpl: s3a-file-system metrics system started\n",
      "2025-12-03 18:05:51,181 INFO tools.SimpleCopyListing: Paths (files+dirs) cnt = 5; dirCnt = 1\n",
      "2025-12-03 18:05:51,181 INFO tools.SimpleCopyListing: Build file listing completed.\n",
      "2025-12-03 18:05:51,183 INFO Configuration.deprecation: io.sort.mb is deprecated. Instead, use mapreduce.task.io.sort.mb\n",
      "2025-12-03 18:05:51,183 INFO Configuration.deprecation: io.sort.factor is deprecated. Instead, use mapreduce.task.io.sort.factor\n",
      "2025-12-03 18:05:51,211 INFO tools.DistCp: Number of paths in the copy list: 5\n",
      "2025-12-03 18:05:51,217 INFO tools.DistCp: Number of paths in the copy list: 5\n",
      "2025-12-03 18:05:51,222 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
      "2025-12-03 18:05:51,266 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "2025-12-03 18:05:51,406 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1475576820_0001\n",
      "2025-12-03 18:05:51,407 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
      "2025-12-03 18:05:51,542 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "2025-12-03 18:05:51,542 INFO tools.DistCp: DistCp job-id: job_local1475576820_0001\n",
      "2025-12-03 18:05:51,543 INFO mapreduce.Job: Running job: job_local1475576820_0001\n",
      "2025-12-03 18:05:51,544 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "2025-12-03 18:05:51,550 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-12-03 18:05:51,551 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-12-03 18:05:51,552 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.tools.mapred.CopyCommitter\n",
      "2025-12-03 18:05:51,602 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "2025-12-03 18:05:51,603 INFO mapred.LocalJobRunner: Starting task: attempt_local1475576820_0001_m_000000_0\n",
      "2025-12-03 18:05:51,630 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-12-03 18:05:51,630 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-12-03 18:05:51,650 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
      "2025-12-03 18:05:51,653 INFO mapred.MapTask: Processing split: file:/tmp/hadoop/mapred/staging/ubuntu437999172/.staging/_distcp1438893600/fileList.seq:0+896\n",
      "2025-12-03 18:05:51,659 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
      "2025-12-03 18:05:51,659 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2025-12-03 18:05:51,671 INFO mapred.CopyMapper: Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/ to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data\n",
      "2025-12-03 18:05:51,696 INFO mapred.CopyMapper: Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/train_1000k.csv to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/train_1000k.csv\n",
      "2025-12-03 18:05:51,723 INFO mapred.RetriableFileCopyCommand: Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/train_1000k.csv to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/train_1000k.csv\n",
      "2025-12-03 18:05:51,724 INFO mapred.RetriableFileCopyCommand: Creating temp file: hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785151724\n",
      "2025-12-03 18:05:51,724 INFO mapred.RetriableFileCopyCommand: Writing to temporary target file path hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785151724\n",
      "2025-12-03 18:05:52,545 INFO mapreduce.Job: Job job_local1475576820_0001 running in uber mode : false\n",
      "2025-12-03 18:05:52,546 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "2025-12-03 18:05:52,897 INFO mapred.RetriableFileCopyCommand: Renaming temporary target file path hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785151724 to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/train_1000k.csv\n",
      "2025-12-03 18:05:52,912 INFO mapred.RetriableFileCopyCommand: Completed writing hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/train_1000k.csv (53700464 bytes)\n",
      "2025-12-03 18:05:52,916 INFO mapred.CopyMapper: Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/lectures.csv to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/lectures.csv\n",
      "2025-12-03 18:05:52,966 INFO mapred.RetriableFileCopyCommand: Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/lectures.csv to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/lectures.csv\n",
      "2025-12-03 18:05:52,966 INFO mapred.RetriableFileCopyCommand: Creating temp file: hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785152966\n",
      "2025-12-03 18:05:52,966 INFO mapred.RetriableFileCopyCommand: Writing to temporary target file path hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785152966\n",
      "2025-12-03 18:05:53,018 INFO mapred.RetriableFileCopyCommand: Renaming temporary target file path hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785152966 to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/lectures.csv\n",
      "2025-12-03 18:05:53,023 INFO mapred.RetriableFileCopyCommand: Completed writing hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/lectures.csv (9703 bytes)\n",
      "2025-12-03 18:05:53,025 INFO mapred.CopyMapper: Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/questions.csv to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/questions.csv\n",
      "2025-12-03 18:05:53,038 INFO mapred.RetriableFileCopyCommand: Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/questions.csv to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/questions.csv\n",
      "2025-12-03 18:05:53,038 INFO mapred.RetriableFileCopyCommand: Creating temp file: hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785153038\n",
      "2025-12-03 18:05:53,038 INFO mapred.RetriableFileCopyCommand: Writing to temporary target file path hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785153038\n",
      "2025-12-03 18:05:53,483 INFO mapred.RetriableFileCopyCommand: Renaming temporary target file path hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785153038 to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/questions.csv\n",
      "2025-12-03 18:05:53,487 INFO mapred.RetriableFileCopyCommand: Completed writing hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/questions.csv (296161 bytes)\n",
      "2025-12-03 18:05:53,488 INFO mapred.CopyMapper: Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt\n",
      "2025-12-03 18:05:53,533 INFO mapred.RetriableFileCopyCommand: Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt\n",
      "2025-12-03 18:05:53,533 INFO mapred.RetriableFileCopyCommand: Creating temp file: hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785153533\n",
      "2025-12-03 18:05:53,533 INFO mapred.RetriableFileCopyCommand: Writing to temporary target file path hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785153533\n",
      "2025-12-03 18:06:03,640 INFO mapred.LocalJobRunner: 19.8% Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt [592.0M/2.9G] > map\n",
      "2025-12-03 18:06:04,553 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "2025-12-03 18:06:09,641 INFO mapred.LocalJobRunner: 32.4% Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt [968.0M/2.9G] > map\n",
      "2025-12-03 18:06:15,642 INFO mapred.LocalJobRunner: 44.9% Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt [1.3G/2.9G] > map\n",
      "2025-12-03 18:06:21,644 INFO mapred.LocalJobRunner: 54.9% Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt [1.6G/2.9G] > map\n",
      "2025-12-03 18:06:27,645 INFO mapred.LocalJobRunner: 66.4% Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt [1.9G/2.9G] > map\n",
      "2025-12-03 18:06:33,645 INFO mapred.LocalJobRunner: 73.4% Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt [2.1G/2.9G] > map\n",
      "2025-12-03 18:06:39,646 INFO mapred.LocalJobRunner: 84.1% Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt [2.5G/2.9G] > map\n",
      "2025-12-03 18:06:45,647 INFO mapred.LocalJobRunner: 96.5% Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt [2.8G/2.9G] > map\n",
      "2025-12-03 18:06:47,252 INFO mapred.RetriableFileCopyCommand: Renaming temporary target file path hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/.distcp.tmp.attempt_local1475576820_0001_m_000000_0.1764785153533 to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt\n",
      "2025-12-03 18:06:47,260 INFO mapred.RetriableFileCopyCommand: Completed writing hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt (3136657969 bytes)\n",
      "2025-12-03 18:06:47,261 INFO mapred.LocalJobRunner: 96.5% Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt [2.8G/2.9G] > map\n",
      "2025-12-03 18:06:47,262 INFO mapred.Task: Task:attempt_local1475576820_0001_m_000000_0 is done. And is in the process of committing\n",
      "2025-12-03 18:06:47,263 INFO mapred.LocalJobRunner: 96.5% Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt [2.8G/2.9G] > map\n",
      "2025-12-03 18:06:47,263 INFO mapred.Task: Task attempt_local1475576820_0001_m_000000_0 is allowed to commit now\n",
      "2025-12-03 18:06:47,264 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1475576820_0001_m_000000_0' to file:/tmp/hadoop/mapred/staging/ubuntu437999172/.staging/_distcp1438893600/_logs\n",
      "2025-12-03 18:06:47,265 INFO mapred.LocalJobRunner: 100.0% Copying s3a://otus-bucket-b1gcq77orp7o97v8lnkm/2022-11-04.txt to hdfs://rc1a-dataproc-m-h0l0vmjdoi61uau8.mdb.yandexcloud.net/user/ubuntu/data/2022-11-04.txt [2.9G/2.9G]\n",
      "2025-12-03 18:06:47,265 INFO mapred.Task: Task 'attempt_local1475576820_0001_m_000000_0' done.\n",
      "2025-12-03 18:06:47,273 INFO mapred.Task: Final Counters for attempt_local1475576820_0001_m_000000_0: Counters: 31\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=191077\n",
      "\t\tFILE: Number of bytes written=749867\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=0\n",
      "\t\tHDFS: Number of bytes written=3190664297\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=13\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tS3A: Number of bytes read=3190664297\n",
      "\t\tS3A: Number of bytes written=0\n",
      "\t\tS3A: Number of read operations=14\n",
      "\t\tS3A: Number of large read operations=0\n",
      "\t\tS3A: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=0\n",
      "\t\tInput split bytes=152\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=101\n",
      "\t\tTotal committed heap usage (bytes)=649592832\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=932\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8\n",
      "\tDistCp Counters\n",
      "\t\tBandwidth in Btyes=58012078\n",
      "\t\tBytes Copied=3190664297\n",
      "\t\tBytes Expected=3190664297\n",
      "\t\tFiles Copied=4\n",
      "\t\tDIR_COPY=1\n",
      "2025-12-03 18:06:47,273 INFO mapred.LocalJobRunner: Finishing task: attempt_local1475576820_0001_m_000000_0\n",
      "2025-12-03 18:06:47,273 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "2025-12-03 18:06:47,307 INFO mapred.CopyCommitter: Cleaning up temporary work folder: file:/tmp/hadoop/mapred/staging/ubuntu437999172/.staging/_distcp1438893600\n",
      "2025-12-03 18:06:47,570 INFO mapreduce.Job: Job job_local1475576820_0001 completed successfully\n",
      "2025-12-03 18:06:47,577 INFO mapreduce.Job: Counters: 31\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=191077\n",
      "\t\tFILE: Number of bytes written=749867\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=0\n",
      "\t\tHDFS: Number of bytes written=3190664297\n",
      "\t\tHDFS: Number of read operations=16\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=13\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tS3A: Number of bytes read=3190664297\n",
      "\t\tS3A: Number of bytes written=0\n",
      "\t\tS3A: Number of read operations=14\n",
      "\t\tS3A: Number of large read operations=0\n",
      "\t\tS3A: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=5\n",
      "\t\tMap output records=0\n",
      "\t\tInput split bytes=152\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=101\n",
      "\t\tTotal committed heap usage (bytes)=649592832\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=932\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=8\n",
      "\tDistCp Counters\n",
      "\t\tBandwidth in Btyes=58012078\n",
      "\t\tBytes Copied=3190664297\n",
      "\t\tBytes Expected=3190664297\n",
      "\t\tFiles Copied=4\n",
      "\t\tDIR_COPY=1\n",
      "2025-12-03 18:06:47,580 INFO impl.MetricsSystemImpl: Stopping s3a-file-system metrics system...\n",
      "2025-12-03 18:06:47,580 INFO impl.MetricsSystemImpl: s3a-file-system metrics system stopped.\n",
      "2025-12-03 18:06:47,580 INFO impl.MetricsSystemImpl: s3a-file-system metrics system shutdown complete.\n"
     ]
    }
   ],
   "source": [
    "!hadoop distcp s3a://otus-bucket-b1gcq77orp7o97v8lnkm/ /user/ubuntu/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1bf1b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   1 ubuntu hadoop 3136657969 2025-12-03 18:06 /user/ubuntu/data/2022-11-04.txt\n",
      "-rw-r--r--   1 ubuntu hadoop       9703 2025-12-03 18:05 /user/ubuntu/data/lectures.csv\n",
      "-rw-r--r--   1 ubuntu hadoop     296161 2025-12-03 18:05 /user/ubuntu/data/questions.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   53700464 2025-12-03 18:05 /user/ubuntu/data/train_1000k.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/ubuntu/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8582f495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "drwxr-xr-x   - ubuntu hadoop          0 2025-12-03 18:06 data\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61924a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   1 ubuntu hadoop 3136657969 2025-12-03 18:06 data/2022-11-04.txt\n",
      "-rw-r--r--   1 ubuntu hadoop       9703 2025-12-03 18:05 data/lectures.csv\n",
      "-rw-r--r--   1 ubuntu hadoop     296161 2025-12-03 18:05 data/questions.csv\n",
      "-rw-r--r--   1 ubuntu hadoop   53700464 2025-12-03 18:05 data/train_1000k.csv\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b353884b",
   "metadata": {},
   "source": [
    "Создадим SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a851f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "        .builder\n",
    "        .appName(\"OTUS\")\n",
    "        .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7d9429",
   "metadata": {},
   "source": [
    "Загрузим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6b5fb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"data/train_1000k.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed116eb1",
   "metadata": {},
   "source": [
    "Поделим данные на 10 разделов и сохраним в формате parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c61e2248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.99 ms, sys: 0 ns, total: 1.99 ms\n",
      "Wall time: 626 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee16ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "        .repartition(10)\n",
    "        .write\n",
    "        .mode(\"overwrite\")\n",
    "        .parquet(\"data/train.parquet\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b5e8f",
   "metadata": {},
   "source": [
    "Проверим размер сохраненного файла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dae6028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 items\n",
      "-rw-r--r--   1 ubuntu hadoop          0 2025-12-03 18:08 data/train.parquet/_SUCCESS\n",
      "-rw-r--r--   1 ubuntu hadoop      1.8 M 2025-12-03 18:08 data/train.parquet/part-00000-09842bc8-6262-4feb-a900-789d9cb1e6df-c000.snappy.parquet\n",
      "-rw-r--r--   1 ubuntu hadoop      1.8 M 2025-12-03 18:08 data/train.parquet/part-00001-09842bc8-6262-4feb-a900-789d9cb1e6df-c000.snappy.parquet\n",
      "-rw-r--r--   1 ubuntu hadoop      1.8 M 2025-12-03 18:08 data/train.parquet/part-00002-09842bc8-6262-4feb-a900-789d9cb1e6df-c000.snappy.parquet\n",
      "-rw-r--r--   1 ubuntu hadoop      1.8 M 2025-12-03 18:08 data/train.parquet/part-00003-09842bc8-6262-4feb-a900-789d9cb1e6df-c000.snappy.parquet\n",
      "-rw-r--r--   1 ubuntu hadoop      1.8 M 2025-12-03 18:08 data/train.parquet/part-00004-09842bc8-6262-4feb-a900-789d9cb1e6df-c000.snappy.parquet\n",
      "-rw-r--r--   1 ubuntu hadoop      1.8 M 2025-12-03 18:08 data/train.parquet/part-00005-09842bc8-6262-4feb-a900-789d9cb1e6df-c000.snappy.parquet\n",
      "-rw-r--r--   1 ubuntu hadoop      1.8 M 2025-12-03 18:08 data/train.parquet/part-00006-09842bc8-6262-4feb-a900-789d9cb1e6df-c000.snappy.parquet\n",
      "-rw-r--r--   1 ubuntu hadoop      1.8 M 2025-12-03 18:08 data/train.parquet/part-00007-09842bc8-6262-4feb-a900-789d9cb1e6df-c000.snappy.parquet\n",
      "-rw-r--r--   1 ubuntu hadoop      1.8 M 2025-12-03 18:08 data/train.parquet/part-00008-09842bc8-6262-4feb-a900-789d9cb1e6df-c000.snappy.parquet\n",
      "-rw-r--r--   1 ubuntu hadoop      1.8 M 2025-12-03 18:08 data/train.parquet/part-00009-09842bc8-6262-4feb-a900-789d9cb1e6df-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h data/train.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f5332d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_from_parquet = spark.read.parquet(\"data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f983a7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.91 ms, sys: 0 ns, total: 1.91 ms\n",
      "Wall time: 303 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "df_from_parquet.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e98217c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+--------+----------+---------------+-----------------+-----------+------------------+---------------------------+------------------------------+\n",
      "|row_id|  timestamp| user_id|content_id|content_type_id|task_container_id|user_answer|answered_correctly|prior_question_elapsed_time|prior_question_had_explanation|\n",
      "+------+-----------+--------+----------+---------------+-----------------+-----------+------------------+---------------------------+------------------------------+\n",
      "| 62896|   96541616| 1400354|      1087|              0|               94|          3|                 0|                    17000.0|                         false|\n",
      "|104413|   21162902| 2211492|      5262|              0|               95|          0|                 1|                    11000.0|                          true|\n",
      "|537225|    4538020|10854346|      7880|              0|               26|          1|                 1|                    30000.0|                          true|\n",
      "|158202|      95544| 3237407|      1278|              0|                3|          0|                 0|                    23000.0|                         false|\n",
      "| 75576|45311590335| 1567938|       768|              0|             1665|          0|                 1|                    17000.0|                          true|\n",
      "+------+-----------+--------+----------+---------------+-----------------+-----------+------------------+---------------------------+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_from_parquet.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
